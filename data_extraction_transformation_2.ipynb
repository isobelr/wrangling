{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1: Task 2\n",
    "#### Student Name: Isobel Rowe\n",
    "#### Student ID: 30042585\n",
    "\n",
    "Date: 10/04/2019\n",
    "\n",
    "Version: 2.0\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* PyPDF2\n",
    "* re\n",
    "* nltk\n",
    "* nltk.collocations\n",
    "* nltk.tokenize \n",
    "* Pandas\n",
    "* MWETokenizer from nltk.tokenize\n",
    "* PorterStemmer from nltk.stem\n",
    "* itertools\n",
    "* CountVectorizer from sklearn\n",
    "\n",
    "## 1. Introduction\n",
    "This task of assignment 1 focuses on extracting data from PDF files and transforming it into a vector space model. There are a number of steps involved in this process, which can broadly be outlined by the following:\n",
    "\n",
    "1. Extract the unit information from the PDF file.\n",
    "2. Process the unit information and transform into the format outlined by the specifications.\n",
    "3. Generate the output - the vocabulary file, and the count vector file.\n",
    "\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from itertools import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Opening, reading and extracting data\n",
    "\n",
    "Firstly, the content of the PDF needs to be extracted and stored in a format that can be used for processing. The PyPDF2 library is used for extraction, and the content is written to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening and reading the PDF file\n",
    "pdf_file = open('30042585.pdf', 'rb')\n",
    "pdf_read = PyPDF2.PdfFileReader(pdf_file)\n",
    "\n",
    "#Creating an output file \n",
    "extract = open('extractedpdf.txt', 'w')\n",
    "\n",
    "#Looping over contents of PDF reader and writing to the extract file.\n",
    "for i in range(pdf_read.getNumPages()):\n",
    "    page = pdf_read.getPage(i)\n",
    "    page_content = page.extractText()\n",
    "    extract.write(page_content)\n",
    "\n",
    "#Closing the output file\n",
    "extract.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit ID retrieval\n",
    "Next, the unit ID's are extracted from the extracted PDF content file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of unit ID:  200\n",
      "['ATS1092', 'VCO1303', 'ATS2791', 'DWG3516', 'BFC3440', 'ATS2791', 'CHE3171', 'IMM2011', 'BFX5018']\n"
     ]
    }
   ],
   "source": [
    "#Reading textfile in as string \n",
    "with open('extractedpdf.txt', 'r') as file:\n",
    "    unitid1 = re.findall('^\\w{3,4}\\d{4}\\n', file.read(), flags=re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "# Removing the /n characters    \n",
    "unitid = [x[:-1] for x in unitid1]\n",
    "\n",
    "#Checking the length of the list to ensure that the regex was a success\n",
    "print(\"Length of unit ID: \", len(unitid))\n",
    "print(unitid[1:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case normalisation\n",
    "Next, case normalisation. Normally, this would be completed after tokenisation, however, as we want to keep the capital letter that appear in the middle of sentences, it's best to do it now. Another possibility is to use NLTK sentence tokeniser, but, in doing so, sentences starting with \"\\['\" in the 'Outcomes' section would be left capitalised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "this unit introduces students to the technological,\n",
      "social, economic and political forces driving the\n",
      "development, and adoption of new media and\n",
      "communications technologies. it examines case\n",
      "studies of when 'old technologies were new' such as\n",
      "the telegraph and radio as well as the social shaping\n",
      "of very recent examples of new media, such as\n",
      "Facebook, Sina weibo, Qzone, Renren and Twitter.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Opening and reading the file\n",
    "textfile = open('extractedpdf.txt', 'r+')\n",
    "text = textfile.read()\n",
    "\n",
    "# Defining the regex patterns\n",
    "pattern_a = r'(?<=[.?!]\\s)(\\w+)' #Matches everything after a sentence stopper - '.', '?', and '!'\n",
    "pattern_b = r'\\[\\'\\w' #Matches everything in the content square bracket\n",
    "pattern_c = r'(?<=^\\w{3}\\d{4}\\n)\\w' #Matches everything after the unit ID in the synopsis section\n",
    "\n",
    "# Replacing the capitals for lowercase\n",
    "for f in re.findall(pattern_a, text):\n",
    "    text = text.replace(f, f.lower())\n",
    "for f in re.findall(pattern_b, text):\n",
    "    text = text.replace(f, f.lower())   \n",
    "for f in re.findall(pattern_c, text):\n",
    "    text = text.replace(f, f.lower())  \n",
    "    \n",
    "# Validation\n",
    "print(text[31:424])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content retrieval\n",
    "The information for each unit is extracted from the textfile and stored in a list called 'content'. A regular expression is used here to gather everything in between the unitcode and the final square bracket in the 'Outcomes' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  200\n"
     ]
    }
   ],
   "source": [
    "content = re.findall('(?<=\\w{3}\\d{4}\\n)(.*?)(?=\\])', text, flags = re.MULTILINE|re.DOTALL)\n",
    "print(\"Length: \", len(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Processing the data\n",
    "\n",
    "### Tokenisation\n",
    "Tokenisation, which is the process of splitting sentences up into 'tokens' where ever there is a space, is started using RegexpTokenizer and the supplied regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  200\n",
      "['this', 'unit', 'is', 'for', 'students', 'with', 'little', 'or', 'no', 'knowledge', 'of', 'the', 'language', 'this', 'unit', 'consists', 'of', 'two', 'components', 'component', '1', 'Language', 'a', 'communicatively', 'oriented', 'German', 'language', 'course', 'designed', 'for', 'all-round', 'development', 'in', 'the', 'language', 'component', '2', 'this', 'component', 'will', 'familiarise', 'students', 'with', 'the', 'history', 'culture', 'and', 'the', 'socio-economic', 'conditions', 'of', 'the', 'German-speaking', 'countries', 'na']\n"
     ]
    }
   ],
   "source": [
    "#Creating empty list\n",
    "tokenised = []\n",
    "\n",
    "#Defining the tokeniser\n",
    "tokeniser = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "\n",
    "#Looping over the list and tokenising\n",
    "for element in content:\n",
    "    tokenise = tokeniser.tokenize(str(element))\n",
    "    tokenised.append(tokenise)\n",
    "\n",
    "#Checking the length\n",
    "print(\"Length: \", len(tokenised))\n",
    "print(tokenised[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing words < three characters\n",
    "Now that everything has been extracted from the PDF and stored in lists, processing will start with the removal of words that are less than 3 characters long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "['this', 'unit', 'for', 'students', 'with', 'little', 'knowledge', 'the', 'language', 'this', 'unit', 'consists', 'two', 'components', 'component', 'Language', 'communicatively', 'oriented', 'German', 'language', 'course', 'designed', 'for', 'all-round', 'development', 'the', 'language', 'component', 'this', 'component', 'will', 'familiarise', 'students', 'with', 'the', 'history', 'culture', 'and', 'the', 'socio-economic', 'conditions', 'the', 'German-speaking', 'countries']\n"
     ]
    }
   ],
   "source": [
    "#Creating a new nested list\n",
    "removed3 = []\n",
    "\n",
    "# Looping over and appending words which are more than 2 characters\n",
    "for each in tokenised:\n",
    "    temp = []\n",
    "    for each2 in each:\n",
    "        if(len(each2)>2):\n",
    "            temp.append(each2)\n",
    "    removed3.append(temp)  \n",
    "\n",
    "#Validation\n",
    "print(len(removed3))\n",
    "print(removed3[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe formatting\n",
    "From here, I decided that putting the unit ID and information content into a Pandas dataframe would be the best fit for the folowing sections of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unitid</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATS2436</td>\n",
       "      <td>[this, unit, introduces, students, the, techno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATS1092</td>\n",
       "      <td>[this, unit, for, students, with, little, know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VCO1303</td>\n",
       "      <td>[this, subject, students, will, study, the, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATS2791</td>\n",
       "      <td>[this, unit, provides, detailed, exploration, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DWG3516</td>\n",
       "      <td>[this, unit, provides, students, art, and, des...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    unitid                                             tokens\n",
       "0  ATS2436  [this, unit, introduces, students, the, techno...\n",
       "1  ATS1092  [this, unit, for, students, with, little, know...\n",
       "2  VCO1303  [this, subject, students, will, study, the, wo...\n",
       "3  ATS2791  [this, unit, provides, detailed, exploration, ...\n",
       "4  DWG3516  [this, unit, provides, students, art, and, des..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the dataframe\n",
    "tokens_df = pd.DataFrame(list(zip(unitid, removed3)))\n",
    "tokens_df.columns = ['unitid','tokens']\n",
    "\n",
    "# Verification\n",
    "print(\"Length: \", len(tokens_df))\n",
    "tokens_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram\n",
    "For the bigram generation, I used NLTK and the PMI filter. I applied a frequency filter so that only the 'meaningful' bigrams would be captured. Without this, a lot of unwanted bigrams were captured, such as dates, for example: ('July', '1993')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[('concrete', 'slabs'), ('concise', 'accurate'), ('reinforced', 'concrete'), ('criminal', 'justice'), ('clear', 'concise'), ('interior', 'architecture'), ('drug', 'action'), ('activities', 'dealing'), ('under', 'pressure')]\n"
     ]
    }
   ],
   "source": [
    "# Initialising the bigram\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(list(chain.from_iterable(tokens_df.tokens)))\n",
    "\n",
    "# Applying the frequency filter\n",
    "finder.apply_freq_filter(5)\n",
    "\n",
    "# Finding the bigrams\n",
    "bigrams = finder.nbest(bigram_measures.pmi, 200)\n",
    "\n",
    "print(len(bigrams))\n",
    "print(bigrams[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the bigrams need to be re-tokenised into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25007, 2)\n",
      "          token   unitid\n",
      "0          this  ACC2200\n",
      "1  introductory  ACC2200\n",
      "2    management  ACC2200\n",
      "3    accounting  ACC2200\n",
      "4  unit_focuses  ACC2200\n"
     ]
    }
   ],
   "source": [
    "# Creating empty lists\n",
    "token_bigram = []\n",
    "unit_id_bigram = []\n",
    "# Definfing tokeniser with bigrams\n",
    "tokenizer = MWETokenizer(bigrams)\n",
    "\n",
    "# Grouping by unit ID, and re-tokenising\n",
    "for name, group in tokens_df.groupby(['unitid']):\n",
    "    token_list = list(chain.from_iterable(group.tokens))\n",
    "    tokens_bigrams = tokenizer.tokenize(token_list)\n",
    "    token_bigram += tokens_bigrams # get new list of tokens\n",
    "    unit_id_bigram += ([name] * len(tokens_bigrams)) # with their corresponding patent ID\n",
    "    \n",
    "# Creating a dictioanry of tokens and unit id\n",
    "bigram_token_dict = {}\n",
    "bigram_token_dict['token'] = token_bigram\n",
    "bigram_token_dict['unitid'] = unit_id_bigram\n",
    "\n",
    "# Creating new data frame with the dictionary\n",
    "tokens_bigram_df = pd.DataFrame(bigram_token_dict)\n",
    "\n",
    "# Validation\n",
    "print(tokens_bigram_df.shape)\n",
    "print(tokens_bigram_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "Next, stopword removal, which essentially removes all of the 'filler' words - like 'and', 'the' etc. This process involves using the provided stopword file and filtering out every word in the 'token' column of the dataframe which occurs in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18167, 2)\n",
      "          token   unitid\n",
      "1  introductory  ACC2200\n",
      "2    management  ACC2200\n",
      "3    accounting  ACC2200\n",
      "4  unit_focuses  ACC2200\n",
      "6         types  ACC2200\n"
     ]
    }
   ],
   "source": [
    "# Opening stopword file\n",
    "stopword_file=open('stopwords_en.txt',\"r\",encoding=\"utf8\")\n",
    "# Reading and splitting into list\n",
    "stop_list=stopword_file.read().split('\\n')\n",
    "\n",
    "# Filtering out the tokens that appear in the stopword list\n",
    "tokens_stops_df = tokens_bigram_df[~tokens_bigram_df.token.isin(stop_list)]\n",
    "\n",
    "# Checking shape\n",
    "print(tokens_stops_df.shape)\n",
    "print(tokens_stops_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare Token Removal\n",
    "\n",
    "This step involves removing tokens that appear in 5% (ie. 10) or less of the units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6680, 2)\n",
      "          token   unitid\n",
      "2    management  ACC2200\n",
      "8   information  ACC2200\n",
      "18     planning  ACC2200\n",
      "20      control  ACC2200\n",
      "32   techniques  ACC2200\n"
     ]
    }
   ],
   "source": [
    "rare_words = []\n",
    "\n",
    "# Grouping by token\n",
    "for name, group in tokens_stops_df.groupby(['token']): \n",
    "    # If the token appears in < 10 units\n",
    "    if len(set(group.unitid)) <= 10: \n",
    "        rare_words.append(name)\n",
    "        \n",
    "# Filter the rare words\n",
    "tokens_norare_df = tokens_stops_df[~tokens_stops_df.token.isin(rare_words)]\n",
    "\n",
    "print(tokens_norare_df.shape)\n",
    "print(tokens_norare_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming involves truncating words to their 'base' form. For instance: 'universal' transforms to 'univ'. Here, we're using the Porter stemmer, as provided by the NLTK module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6680, 2)\n",
      "       token   unitid\n",
      "2      manag  ACC2200\n",
      "8     inform  ACC2200\n",
      "18      plan  ACC2200\n",
      "20   control  ACC2200\n",
      "32  techniqu  ACC2200\n"
     ]
    }
   ],
   "source": [
    "# Defining the Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Applying stemmer on the token column of dataframe\n",
    "tokens_norare_df.token = tokens_norare_df['token'].apply(stemmer.stem)\n",
    "# Defining new dataframe\n",
    "tokens_stemmed_df = tokens_norare_df\n",
    "\n",
    "# Validating\n",
    "print(tokens_stemmed_df.shape)\n",
    "print(tokens_stemmed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Output\n",
    "\n",
    "### Vocabulary\n",
    "The final step is to generate the output files for the vocabulary and the vector space model. First is the vocabulary, where we use CountVectorisor from scikit-learn which converts the processed text into a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use count vectorizer to count the words and remove context dependent stopwords with 95% frequency   \n",
    "vectorizer = CountVectorizer(analyzer = 'word', max_df=0.95)\n",
    "vectorizerobject = vectorizer.fit_transform(tokens_stemmed_df.token)\n",
    "\n",
    "# Initialising vectoriser to find vocab\n",
    "vocab=vectorizer.get_feature_names()\n",
    "output=vectorizer.vocabulary_\n",
    "\n",
    "# Defining keylist and sorting in alphabetical order\n",
    "keys = list(output.keys()) \n",
    "keys.sort()\n",
    "\n",
    "# Creating the string to write to file\n",
    "vocabstring = '' \n",
    "for key in keys:\n",
    "    vocabstring = vocabstring + str(key) + ':' + str(output[key]) + '\\n' \n",
    "    \n",
    "# Writing the string to the file\n",
    "with open('30042585_vocab.txt', 'w') as vocaboutput:\n",
    "    vocaboutput.write(vocabstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector space model\n",
    "\n",
    "Finally, the vector space model. The first step in producing this is to create a new dataframe, with the frequency count of each token per unit ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    unitid    token  frequency\n",
      "0  ACC2200   analys          2\n",
      "1  ACC2200  analysi          1\n",
      "2  ACC2200  context          1\n",
      "3  ACC2200  control          1\n",
      "4  ACC2200    decis          2\n",
      "(4409, 3)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataframe\n",
    "tokens_freq_df = pd.DataFrame(\n",
    "                        {'frequency': tokens_stemmed_df.groupby(['unitid', 'token']).size()}\n",
    "                    ).reset_index()\n",
    "# Validate\n",
    "print(tokens_freq_df.head())\n",
    "print(tokens_freq_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to map the token ID (from the vocabulary dictionary) to the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    unitid    token  frequency  tokenid\n",
      "0  ACC2200   analys          2        3\n",
      "1  ACC2200  analysi          1        4\n",
      "2  ACC2200  context          1       33\n",
      "3  ACC2200  control          1       34\n",
      "4  ACC2200    decis          2       41\n",
      "(4409, 4)\n"
     ]
    }
   ],
   "source": [
    "# Creating dictionary with word:word_id\n",
    "vocabulary_dict = {token:index for index, token in enumerate(keys)}\n",
    "\n",
    "# Mapping the token ID \n",
    "tokens_freq_df['tokenid'] = tokens_freq_df.token.map(vocabulary_dict)\n",
    "\n",
    "# Validating\n",
    "print(tokens_freq_df.head())\n",
    "print(tokens_freq_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, writing the vector space model a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_writer = open('30042585_countVec.txt', 'w')\n",
    "\n",
    "# Grouping by unit ID\n",
    "for unitid, group in tokens_freq_df.groupby(['unitid']):\n",
    "    # Writing in the specified format to the file\n",
    "    a_writer.write(unitid + ','+ ','.join(group.tokenid.map(str) + ':' + group.frequency.map(str)) + '\\n')\n",
    "    \n",
    "a_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "This task has demonstrated the basics of text pre-processing in the Python. The main outcomes achieved while executing this task were: \n",
    "\n",
    "* extracting data from PDF files, \n",
    "* natural language processing with:\n",
    "    * word tokenisation, \n",
    "    * normalisation\n",
    "    * the removal of stopwords, rare tokens, and tokens less than three characters long,\n",
    "    * bigram generation, and\n",
    "    * stemming.\n",
    "* creation of vocabulary and sparse representations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References\n",
    "\n",
    "The Python Software Foundation. (2019). *'9.7. itertools â€” Functions creating iterators for efficient looping'*. Retrieved from https://docs.python.org/2/library/itertools.html#module-itertools\n",
    "\n",
    "NLTK Project. (2017). NLTK 3.0 documentation: *nltk.tokenize.mwe module*. Retrieved from http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.mwe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
